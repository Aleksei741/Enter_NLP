{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "\n",
    "Исключим стоп-слова с помощью stop_words='english'.\n",
    "\n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('C:/Users/a.kraev/Enter_NLP/Lesson_1/preprocessed_tweets.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love yoyou take with yoyou all the time ...</td>\n",
       "      <td>[model, love, yoyou, take, with, yoyou, all, t...</td>\n",
       "      <td>[model, love, yoyou, take, yoyou, time, yoyour]</td>\n",
       "      <td>[model, love, yoyou, take, yoyou, time, yoyour]</td>\n",
       "      <td>[model, love, yoyou, take, yoyou, time, yoyour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "3  model love yoyou take with yoyou all the time ...   \n",
       "4                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, yoyou, take, with, yoyou, all, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3    [model, love, yoyou, take, yoyou, time, yoyour]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "3    [model, love, yoyou, take, yoyou, time, yoyour]   \n",
       "4                        [factsguid, societi, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...  \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...  \n",
       "2                                  [bihday, majesti]  \n",
       "3    [model, love, yoyou, take, yoyou, time, yoyour]  \n",
       "4                        [factsguid, societi, motiv]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['father', 'dysfunct', 'selfish', 'drag', 'kid', 'dysfunct', 'run']),\n",
       "       list(['thank', 'lyft', 'credit', 'use', 'caus', 'offer', 'wheelchair', 'van', 'pdx', 'disapoint', 'getthank']),\n",
       "       list(['bihday', 'majesti']), ...,\n",
       "       list(['hillari', 'campaign', 'today', 'ohioomg', 'use', 'word', 'like', 'assetsli', 'never', 'clinton', 'say', 'theeword', 'radic']),\n",
       "       list(['happi', 'work', 'confer', 'right', 'mindset', 'lead', 'cultureofdevelop', 'organ', 'work', 'mindset']),\n",
       "       list(['song', 'glad', 'free', 'download', 'shoegaz', 'newmus', 'newsong'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_stemmed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stemmed = df['tweet_stemmed'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yoyou</th>\n",
       "      <th>yoyour</th>\n",
       "      <th>yoyoy</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "3    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "4    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yo  yoga  york  young  youtub  yoyou  yoyour  yoyoy  yr  yummi  \n",
       "0   0     0     0      0       0      0       0      0   0      0  \n",
       "1   0     0     0      0       0      0       0      0   0      0  \n",
       "2   0     0     0      0       0      0       0      0   0      0  \n",
       "3   0     0     0      0       0      2       1      0   0      0  \n",
       "4   0     0     0      0       0      0       0      0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english',\n",
    "                                   max_df=0.9,\n",
    "                                   max_features=1000)\n",
    "\n",
    "#bag_of_words = count_vectorizer.fit_transform(df['tweet_stemmed'].values)\n",
    "bag_of_words = count_vectorizer.fit_transform(tweet_stemmed)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_bag_of_words = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)\n",
    "df_bag_of_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yoyou</th>\n",
       "      <th>yoyour</th>\n",
       "      <th>yoyoy</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "3    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "4    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yo  yoga  york  young  youtub  yoyou  yoyour  yoyoy  yr  yummi  \n",
       "0   0     0     0      0       0      0       0      0   0      0  \n",
       "1   0     0     0      0       0      0       0      0   0      0  \n",
       "2   0     0     0      0       0      0       0      0   0      0  \n",
       "3   0     0     0      0       0      2       1      0   0      0  \n",
       "4   0     0     0      0       0      0       0      0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_lemmatized = df['tweet_lemmatized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "bag_of_words = count_vectorizer.fit_transform(tweet_lemmatized)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "df_bag_of_words = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)\n",
    "df_bag_of_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "\n",
    "Исключим стоп-слова с помощью stop_words='english'.\n",
    "\n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yoyou</th>\n",
       "      <th>yoyour</th>\n",
       "      <th>yoyoy</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.743214</td>\n",
       "      <td>0.391907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "    yo  yoga  york  young  youtub     yoyou    yoyour  yoyoy   yr  yummi  \n",
       "0  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "1  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "2  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "3  0.0   0.0   0.0    0.0     0.0  0.743214  0.391907    0.0  0.0    0.0  \n",
       "4  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#document = [\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                                   max_df=0.9,\n",
    "                                   max_features=1000)\n",
    "\n",
    "values = tfidf_vectorizer.fit_transform(tweet_stemmed)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "df_bag_of_words = pd.DataFrame(values.toarray(), columns = feature_names)\n",
    "df_bag_of_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yoyou</th>\n",
       "      <th>yoyour</th>\n",
       "      <th>yoyoy</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.743214</td>\n",
       "      <td>0.391907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "    yo  yoga  york  young  youtub     yoyou    yoyour  yoyoy   yr  yummi  \n",
       "0  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "1  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "2  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "3  0.0   0.0   0.0    0.0     0.0  0.743214  0.391907    0.0  0.0    0.0  \n",
       "4  0.0   0.0   0.0    0.0     0.0  0.000000  0.000000    0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = tfidf_vectorizer.fit_transform(tweet_lemmatized)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "df_bag_of_words = pd.DataFrame(values.toarray(), columns = feature_names)\n",
    "df_bag_of_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "\n",
    "Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "\n",
    "Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "\n",
    "Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107102 sha256=33b1ee59bc7819379a2cab2305d4faa737cb90de3da0f324e8c4b7fb8b7af8d2\n",
      "  Stored in directory: c:\\users\\a.kraev\\appdata\\local\\pip\\cache\\wheels\\11\\73\\9a\\f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.21\n",
      "    Uninstalling Cython-0.29.21:\n",
      "      Successfully uninstalled Cython-0.29.21\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_token = df['tweet_token'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982343, 11591280)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v = Word2Vec(sentences=tweet_token, \n",
    "                     size=200, \n",
    "                     window=5,\n",
    "                     min_count=2, \n",
    "                     sg=1, \n",
    "                     hs=0, \n",
    "                     negative=10, \n",
    "                     workers=32, \n",
    "                     seed=34)\n",
    "                \n",
    "model_w2v.train(tweet_token, total_examples=len(tweet_token), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-d1869f162b1d>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model_w2v.most_similar(positive=['dinner'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bihdaydinner', 0.5519595146179199),\n",
       " ('bolognese', 0.5411607027053833),\n",
       " ('spaghetti', 0.5368869304656982),\n",
       " ('shawarma', 0.53103107213974),\n",
       " ('cookout', 0.5233859419822693),\n",
       " ('lamb', 0.520552396774292),\n",
       " ('whoopppp', 0.5192306041717529),\n",
       " ('hamburger', 0.5185060501098633),\n",
       " ('burritos', 0.5141758322715759),\n",
       " ('tacotuesday', 0.5091941952705383)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.most_similar(positive=['dinner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-4941399215e7>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model_w2v.most_similar(positive=['trump'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('donald', 0.552751898765564),\n",
       " ('suppoer', 0.5360764265060425),\n",
       " ('dumptrump', 0.5295127034187317),\n",
       " ('fuhered', 0.5190867185592651),\n",
       " ('trumps', 0.5048829913139343),\n",
       " ('wonthey', 0.5029566287994385),\n",
       " ('unfavorability', 0.5024405717849731),\n",
       " ('misogny', 0.5019585490226746),\n",
       " ('chopra', 0.5016833543777466),\n",
       " ('trumptrain', 0.5014483332633972)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.most_similar(positive=['trump'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-7b73201d08bc>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model_w2v['food']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.50116456,  0.2888268 ,  0.5696338 ,  0.07132174, -0.3276983 ,\n",
       "        0.00295825, -0.6324954 , -0.14509767,  0.8135986 ,  0.18629423,\n",
       "        0.56230056, -0.31309864, -0.384345  , -0.8896136 ,  0.25878698,\n",
       "        0.48804057, -0.26635772,  0.35222256,  0.29845262,  0.18818268,\n",
       "        0.7499717 , -0.10954446, -0.10032492,  0.21296756, -0.44483352,\n",
       "       -0.42685056,  0.9078604 , -0.3880988 ,  1.0170097 ,  0.17432252,\n",
       "        0.4929038 , -0.3255932 , -0.69221157, -0.81337374,  0.39071733,\n",
       "       -0.5419094 , -0.10863677, -0.19672984, -0.79090667,  0.29875886,\n",
       "       -0.18347584, -0.03323686, -0.33881864,  0.15480952,  0.7633341 ,\n",
       "        0.00215299,  0.20568897, -0.5381058 , -0.18115313,  0.44667292,\n",
       "       -0.23121065, -0.12385497,  0.06232333,  0.19657663, -0.78329235,\n",
       "        0.53696233,  0.84765303,  0.7156678 ,  0.07672892, -0.57932377,\n",
       "       -0.35788682,  0.37800452, -0.44049084, -0.21311104,  0.13437043,\n",
       "        0.37020338,  0.46565422,  0.09285545,  0.11941434, -0.06997967,\n",
       "       -0.5069758 ,  0.2928474 , -0.33107644, -0.50655025, -0.33200246,\n",
       "       -0.22050917, -0.9393303 ,  0.04622535, -0.02175883,  0.66148156,\n",
       "        0.30752596,  0.56807965,  0.03166312,  0.15779929,  0.37812707,\n",
       "       -0.00406738, -0.42606935,  0.33249146,  0.12911102, -0.03462507,\n",
       "        0.40124503,  0.1522882 , -0.12701465, -0.3445948 , -0.0496376 ,\n",
       "        0.2023046 ,  0.5130137 ,  0.19265515,  0.2056412 ,  0.29227045,\n",
       "        0.34867164,  0.27239883, -0.84660286,  0.5662717 ,  0.24187894,\n",
       "        0.26907054,  0.00730879,  0.22649597,  0.28831553,  0.01919159,\n",
       "        0.35758644,  0.9988646 , -0.39322197, -0.24017829,  0.07957202,\n",
       "       -0.30501798,  0.737463  ,  0.49855578,  0.09397577, -0.05797294,\n",
       "        0.4837181 , -0.27713624,  0.18136369, -0.04557073,  0.11939386,\n",
       "        0.25720292,  0.2746656 , -0.9764224 ,  0.3618849 ,  0.42163017,\n",
       "       -0.34558323,  0.2441753 , -0.7272886 , -0.12996346,  0.27292395,\n",
       "       -1.124363  , -0.14443335, -0.08882778, -0.67119473, -0.27297166,\n",
       "       -0.51371425, -0.21148352, -0.35283595, -0.41793147, -0.02095076,\n",
       "       -0.31386515,  0.56523037,  0.25206923,  0.14705966,  1.0061321 ,\n",
       "       -0.2986698 ,  0.31239215, -0.5359135 , -0.556211  ,  0.26498133,\n",
       "        0.14277604, -0.1698065 , -0.54141545, -0.3157842 ,  0.70322526,\n",
       "       -0.16566251, -1.1795262 ,  0.30440086, -0.6356277 ,  0.10616486,\n",
       "        0.05026493, -0.7227862 , -0.0170233 ,  0.60953754,  0.31546184,\n",
       "        0.5142528 , -0.32334688, -0.19577074,  0.64580315, -0.53735054,\n",
       "        0.23344105, -0.04891827, -0.19013247, -0.19984408, -0.31846878,\n",
       "       -0.28686598,  0.70494354, -0.2340817 ,  0.07503977, -0.68927765,\n",
       "       -0.47452328,  0.12812506, -0.02231123, -0.24366151,  0.5977272 ,\n",
       "       -0.59194005, -0.00254608, -0.4704074 , -0.11460655,  0.46259022,\n",
       "        0.13258676,  0.20876856,  0.43441975,  0.63623995,  0.03723865],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v['food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size))\n",
    "и поделить финальный вектор на количество слов в твите.\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-897f5097aa3c>:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in model:\n",
      "<ipython-input-44-897f5097aa3c>:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vec += model[word].reshape((1, size))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49159, 200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_Text2Vect(text, model, size=200):\n",
    "    vec = np.zeros((1, size))\n",
    "    norm = 0\n",
    "    \n",
    "    for word in text:\n",
    "        if word in model:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            norm += 1\n",
    "            \n",
    "    if norm > 0:\n",
    "        vec = vec / norm\n",
    "    return vec\n",
    "\n",
    "word_vec_df = df['tweet_token'].apply(lambda x: convert_Text2Vect(x, model_w2v))\n",
    "word_vec_df = np.concatenate(word_vec_df)\n",
    "word_vec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
